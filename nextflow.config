#!/usr/bin/env nextflow

/*
#
#
#  ██████╗ ██╗  ██╗██╗   ██╗██╗      ██████╗ ██████╗ ██╗  ██╗███████╗██████╗ ███████╗
#  ██╔══██╗██║  ██║╚██╗ ██╔╝██║     ██╔═══██╗██╔══██╗██║  ██║██╔════╝██╔══██╗██╔════╝
#  ██████╔╝███████║ ╚████╔╝ ██║     ██║   ██║██████╔╝███████║█████╗  ██████╔╝█████╗
#  ██╔═══╝ ██╔══██║  ╚██╔╝  ██║     ██║   ██║██╔═══╝ ██╔══██║██╔══╝  ██╔══██╗██╔══╝
#  ██║     ██║  ██║   ██║   ███████╗╚██████╔╝██║     ██║  ██║███████╗██║  ██║███████╗
#  ╚═╝     ╚═╝  ╚═╝   ╚═╝   ╚══════╝ ╚═════╝ ╚═╝     ╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝╚══════╝
#
#
# PHYLOPHERE: A Nextflow pipeline including a complete set
# of phylogenetic comparative tools and analyses for Phenome-Genome studies
#
# Github: https://github.com/nozerorma/caastools/nf-phylophere
#
# Author:         Miguel Ramon (miguel.ramon@upf.edu)
#
# File: nextflow.config
#
*/

/*
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * Configuration file for setting up global parameters, process-specific resource requirements,
 * and execution profiles. This ensures optimal resource allocation and flexibility across
 * different compute environments.
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 */

// Redirect logs to stable log dir: $ export NXF_LOG_FILE="log/nextflow.log" (must be set in console before running the script)

// Global default params, used in configs
params {
    // Use singularity flipper. DO NOT MODIFY, it initializes the parameter.
    use_singularity             = use_singularity           ?: false
    use_apptainer               = use_apptainer             ?: false

    
    // Boilerplate options
    outdir                     = outdir                     ?: "$baseDir/Out/Phylophere"
    monochrome_logs            = false
    hok_url                    = null
    help                       = false
    version                    = false
}

// Specify that apptainer/singularity should be used and where the cache dir will be for the images.
// The singularity directive is used in favour of the apptainer one, because currently the apptainer
// variant will pull in (and convert) docker images, instead of using pre-built singularity ones.
// See https://nf-co.re/docs/usage/installation#pipeline-software
// and https://nf-co.re/tools#how-the-singularity-image-downloads-work
// See https://www.nextflow.io/docs/latest/config.html#scope-singularity

// Apptainer added for last Nextflow versions. Singularity is Legacy and will be removed.

profiles {
    singularity {
        params {
            use_singularity         =  true
        }
        process.container = 'docker://miralnso/phylophere:latest'
        singularity.enabled         = true
        singularity.autoMounts      = true
        singularity.cacheDir        = "$baseDir/singularity"
    }
    apptainer {
        params {
            use_apptainer           =  true
        }
        process.container = 'docker://miralnso/phylophere:latest'
        apptainer.enabled         = true
        apptainer.autoMounts      = true
        apptainer.cacheDir        = "$baseDir/apptainer"
    }
}

// Export these variables to prevent local Python/R libraries from conflicting with those in the container

env {
    PYTHONNOUSERSITE = 1
    R_PROFILE_USER   = "/.Rprofile"
    R_ENVIRON_USER   = "/.Renviron"
}

// Timestamp
def trace_timestamp = new Date().format('yyyyMMdd_HH')

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']
cleanup = true

timeline {
    enabled = true
    file    = "${params.outdir}/pipeline_info/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.outdir}/pipeline_info/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    trace.overwrite = true
    file    = "${params.outdir}/pipeline_info/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = true
    file    = "${params.outdir}/pipeline_info/pipeline_dag_${trace_timestamp}.html"
}
tower {
  accessToken = "eyJ0aWQiOiAxMTg2MX0uYmFkMTY1ZDk0MGQxZmU0MTRjOWIyYTdmOThkMjdmZDU0OTQ1OTAzZA=="
  enabled = true
}

manifest {
    name            = 'nf-caastools'
    author          = """Miguel Ramon Alonso"""
    homePage        = 'https://github.com/nozerorma/caastools'
    description     = """Nexflow pipeline for running CAAStools analyses"""
    mainScript      = 'main.nf'
    nextflowVersion = '!>=21.04.0'
    version         = '1.0'
    doi             = ''
}

// Functions to fetch the available CPUs and memory of the current execution node.
// Allows dynamic assignment based on SLURM environment or a fixed default.

def get_allocated_cpus(int node_max_cpu) {
    def maxCpus = System.getenv('SLURM_CPUS_PER_TASK') \
                ?: System.getenv('SLURM_JOB_CPUS_PER_NODE') \
                ?: node_max_cpu
    return maxCpus.toInteger()
}


def get_allocated_mem(int node_max_mem) {
    def memPerCpu = System.getenv('SLURM_MEM_PER_CPU')
    def cpusPerTask = System.getenv('SLURM_CPUS_PER_TASK') \
                    ?: System.getenv('SLURM_JOB_CPUS_PER_NODE')
    if (memPerCpu && cpusPerTask) {
        // Use integer division to compute total memory in GB
        node_max_mem = memPerCpu.toInteger().intdiv(1000) * cpusPerTask.toInteger()
    }
    return "${node_max_mem}.GB"
}

// Helper to cap resource requests
// Compares requested obj against max defined in params

def check_max(obj, type) {
    try {
        switch(type) {
            case 'memory':
                return (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                       ? params.max_memory as nextflow.util.MemoryUnit
                       : obj
            case 'time':
                return (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                       ? params.max_time as nextflow.util.Duration
                       : obj
            case 'cpus':
                return Math.min(obj, params.max_cpus as int)
            default:
                return obj
        }
    } catch(all) {
        println "### WARNING ### Invalid params.${type}='${params[type]}' – using default $obj"
        return obj
    }
}

// Executor configuration: local and slurm
executor {
    local {
        // Dynamically set total cores and memory
        cpus     = get_allocated_cpus(32)
        memory   = get_allocated_mem(64)
    }
    slurm {
        submitRateLimit  = '30/1min'
        queueSize        = 30
        exitReadTimeout  = 7.day
    }
}

// Load shared configurations
includeConfig 'conf/modules.config'
includeConfig 'conf/base.config'
includeConfig 'conf/common.config'
includeConfig 'conf/ct.config'
includeConfig 'conf/rerconverge.config'
includeConfig 'conf/ora.config'

// Profiles

// Queue default toggle


profiles {
    // SLURM cluster auto-selection
    slurm {
        params {
            config_profile_description = 'Marvin Cluster @UPF-CSIC'
            config_profile_contact     = 'miguel.ramon@upf.edu'
            config_profile_url         = 'https://www.ibe.upf-csic.es'
            max_memory                 = 128.GB
            max_cpus                   = 128
            max_time                   = 960.h
        }
        process {
            executor = 'slurm'
        }
    }

    // Ad‑hoc local run
    local {
        params {
            config_profile_description = 'Custom ad-hoc local run'
            config_profile_contact     = 'miguel.ramon@upf.edu'
            config_profile_url         = 'https://www.ibe.upf-csic.es'
            max_memory                 = 64.GB
            max_cpus                   = 32
            max_time                   = 5.day
        }
        process {
            executor = 'local'
        }
    }
}
